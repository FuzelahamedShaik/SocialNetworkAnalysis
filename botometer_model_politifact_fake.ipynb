{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8f1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a3db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load BERT model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set max sequence length\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "class Model:\n",
    "    def load_model(self, load_path):\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        checkpoint = torch.load(load_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f'Model loaded from <== {load_path}')\n",
    "        return model\n",
    "\n",
    "    # predict sentence label , for model 1, (0 prediction refers to bot, 1 human), \n",
    "   \n",
    "  \n",
    "    def predict_hate(self, model, sentence):\n",
    "        tokens = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "        tokens = tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens['input_ids'], token_type_ids=None, attention_mask=tokens['attention_mask'])\n",
    "        logits = outputs[0]\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        return predicted.item()\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "    # Load Model and Evaluate, final out put would be (0 prediction refers to bot, 1 refers to human)\n",
    "        model1 = self.load_model('model_1.pt')\n",
    "\n",
    "        predictions=[]\n",
    "        for post in data:\n",
    "            result1=self.predict_hate(model1, post)\n",
    "            if result1==0:\n",
    "                predictions.append('bot')\n",
    "            else:\n",
    "\n",
    "                predictions.append('human')\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff90987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fuzel Shaik\\AppData\\Local\\Temp\\ipykernel_11940\\2028860103.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['full_text'] = test['full_text'].str.replace(r'http\\S+', 'http')  # Remove URLs while preserving \"http\"\n",
      "C:\\Users\\Fuzel Shaik\\AppData\\Local\\Temp\\ipykernel_11940\\2028860103.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['full_text'] = test['full_text'].str.replace(r'[^\\w\\s#@]', '')  # Remove punctuation except hashtags and mention\n",
      "C:\\Users\\Fuzel Shaik\\AppData\\Local\\Temp\\ipykernel_11940\\2028860103.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['full_text'] = test['full_text'].str.replace(r'\\n', '')  # Remove newline characters\n",
      "C:\\Users\\Fuzel Shaik\\AppData\\Local\\Temp\\ipykernel_11940\\2028860103.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['full_text'] = test['full_text'].str.replace(r'\\r', '')  # Remove line breaks\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== model_1.pt\n",
      "100 done!!\n",
      "200 done!!\n",
      "300 done!!\n",
      "400 done!!\n",
      "500 done!!\n",
      "600 done!!\n",
      "700 done!!\n",
      "800 done!!\n",
      "900 done!!\n",
      "1000 done!!\n",
      "1100 done!!\n",
      "1200 done!!\n",
      "1300 done!!\n",
      "1400 done!!\n",
      "1500 done!!\n",
      "1600 done!!\n",
      "1700 done!!\n",
      "1800 done!!\n",
      "1900 done!!\n",
      "2000 done!!\n",
      "2100 done!!\n",
      "2200 done!!\n",
      "2300 done!!\n",
      "2400 done!!\n",
      "2500 done!!\n",
      "2600 done!!\n",
      "2700 done!!\n",
      "2800 done!!\n",
      "2900 done!!\n",
      "3000 done!!\n"
     ]
    }
   ],
   "source": [
    "# Read your test data (in your data you dont need label column)\n",
    "test = pd.read_csv('poitifact_fake_output_raw_id.csv')\n",
    "\n",
    "## Clean the text as like ths. its important it has to be like this\n",
    "test['full_text'] = test['full_text'].astype(str).str.lower()  # Convert text to lowercase\n",
    "test['full_text'] = test['full_text'].str.replace(r'http\\S+', 'http')  # Remove URLs while preserving \"http\"\n",
    "test['full_text'] = test['full_text'].str.replace(r'[^\\w\\s#@]', '')  # Remove punctuation except hashtags and mention\n",
    "test['full_text'] = test['full_text'].str.replace(r'\\n', '')  # Remove newline characters\n",
    "test['full_text'] = test['full_text'].str.replace(r'\\r', '')  # Remove line breaks\n",
    "test['full_text'] = test['full_text'].astype(str)\n",
    "\n",
    "model1 = model.load_model('model_1.pt')\n",
    "\n",
    "predictions=[]\n",
    "count = 0\n",
    "for post in test['full_text'][:3000]:\n",
    "    result1=model.predict_hate(model1, post)\n",
    "    if result1==0:\n",
    "        predictions.append('bot')\n",
    "    else:\n",
    "        predictions.append('human')\n",
    "    count = count + 1\n",
    "    if count%100 == 0:\n",
    "        print(str(count)+' done!!')\n",
    "if count%100 == 0:\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "#predictions = model.predict_proba(test['full_text'][:100]) # sent your test data for prediction\n",
    "\n",
    "# # you dont need this part since you dont have any label\n",
    "# accuracy = metrics.classification_report(test['label'][:100], predictions, digits=3)\n",
    "# print('Accuracy of model cascade: \\n')\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1065d5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0bf55bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breaking first nfl team declares bankruptcy ov...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>breaking first nfl team declares bankruptcy ov...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>breaking first nfl team declares bankruptcy ov...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breaking first nfl team declares bankruptcy ov...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@lupash7 fantastic</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>ice detainer issued for suspected wine country...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>@covfefeh0mbr3 @breitbartnews http</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>@stevetu37749940 @breitbartnews http</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>ice detainer issued for suspected wine country...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>not sending us their best ice detainer issued ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  label\n",
       "0     breaking first nfl team declares bankruptcy ov...  human\n",
       "1     breaking first nfl team declares bankruptcy ov...  human\n",
       "2     breaking first nfl team declares bankruptcy ov...  human\n",
       "3     breaking first nfl team declares bankruptcy ov...  human\n",
       "4                                   @lupash7 fantastic   human\n",
       "...                                                 ...    ...\n",
       "2995  ice detainer issued for suspected wine country...  human\n",
       "2996                 @covfefeh0mbr3 @breitbartnews http  human\n",
       "2997               @stevetu37749940 @breitbartnews http    bot\n",
       "2998  ice detainer issued for suspected wine country...  human\n",
       "2999  not sending us their best ice detainer issued ...  human\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['Text'] = test['full_text'][:3000]\n",
    "result_df['label'] = predictions\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64834d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('poitifact_fake_botometer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b56e5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bot      1509\n",
       "human    1491\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df['label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
